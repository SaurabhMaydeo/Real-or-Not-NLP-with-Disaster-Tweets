{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece\n",
    "!pip install pdpipe \n",
    "!pip install symspellpy\n",
    "!pip install pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "\n",
    "\n",
    "#data pipeline \n",
    "import pdpipe as pdp\n",
    "\n",
    "#Other Preprocessing\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import string\n",
    "# !pip install symspellpy\n",
    "import pkg_resources\n",
    "from symspellpy.symspellpy import SymSpell\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Contraction Import\n",
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, json, csv\n",
    "import re\n",
    "\n",
    "def remove_wrong_abb(key,value):\n",
    "    if(re.search('it means',value)):\n",
    "        return False;\n",
    "    elif(re.search('\\*',value)):\n",
    "        return False\n",
    "    elif(value==\"\"):\n",
    "        return False\n",
    "    elif(len(key)>7):\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "    \n",
    "resp = requests.get(\"http://www.netlingo.com/acronyms.php\")\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "slangdict= {}\n",
    "key=\"\"\n",
    "value=\"\"\n",
    "for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "    for li in div.findAll('li'):\n",
    "        for a in li.findAll('a'):\n",
    "            key =a.text\n",
    "            value = li.text.split(key)[1]\n",
    "            if(remove_wrong_abb(key,value)):\n",
    "                if(re.search('\\ -or- ',value)):\n",
    "                    pos=re.search('\\ -or- ',value).start()\n",
    "                    slangdict[key]=value[:pos]\n",
    "                else:\n",
    "                    slangdict[key]=value\n",
    "\n",
    "                    \n",
    "# with open('myslang.json', 'w') as f:\n",
    "#     json.dump(slangdict, f, indent=2)\n",
    "\n",
    "w = csv.writer(open(\"myslang.csv\", \"w\"))\n",
    "for key, val in slangdict.items():\n",
    "    w.writerow([key.lower(), val.lower()])\n",
    "    \n",
    "reader = csv.reader(open('myslang.csv', 'r'))\n",
    "abbreviations = {}\n",
    "for row in reader:\n",
    "    k, v = row\n",
    "    abbreviations[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_spell_4space = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell_4space.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "\n",
    "sym_spell_misspelled = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell_misspelled.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell_misspelled.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version Edit: Sonam D.\n",
    "def to_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+|pic.twitter.com\\S+')\n",
    "    return url.sub(r' ',text)\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r' ',text)\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', text)\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(r'@\\w*', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub(r'([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_square_bracket(text):\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_angular_bracket(text):\n",
    "    text = re.sub('\\<.*?\\>+', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_newline(text):\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_words_with_numbers(text):\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    return text\n",
    "    \n",
    "#Version Edit: Susanth D.\n",
    "def hashtag_to_words(text):\n",
    "    text = re.sub(r'##', '#', text)\n",
    "    hash_pattern = re.compile(r\"#\\w*\")\n",
    "    hashtag_list = re.findall(r\"#\\w+\",text)\n",
    "    for hashtag in hashtag_list:\n",
    "        hashtag = re.sub(r'#', '', hashtag)\n",
    "        text = re.sub(hashtag, sym_spell_4space.word_segmentation(hashtag).corrected_string, text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "#     text = re.sub(r'# ', '', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_stopwords(text):\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        if token not in stopwords.words('english'):\n",
    "            textop = textop + token + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def correct_misspelled_with_context(text):\n",
    "    suggestions = sym_spell_misspelled.lookup_compound(text, max_edit_distance=2)\n",
    "    text = str(suggestions[0])\n",
    "    text = re.sub(r', \\d', ' ', text)\n",
    "#     text = remove_numbers(text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def stemming_text(text):\n",
    "    stemmer= PorterStemmer()\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        textop = textop + stemmer.stem(token) + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def lemmatization(text):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        textop = textop + lemmatizer.lemmatize(token) + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def convert_abbrev(word):\n",
    "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def convert_abbrev_in_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [convert_abbrev(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "# def handle_contractions(text):\n",
    "#     text = re.sub(r\"’\", \"'\", text)\n",
    "#     for word in text.split():\n",
    "#         if word.lower() in contractions:\n",
    "#             text = text.replace(word, contractions[word.lower()])\n",
    "#     return(text)\n",
    "\n",
    "#Version Edit: Saurabh M.\n",
    "def removeRepeated(tweet):\n",
    "    prev = ''\n",
    "    tweet_new = ''\n",
    "    for c in tweet:\n",
    "        caps = False\n",
    "        if c.isdigit():\n",
    "            tweet_new += c\n",
    "            continue\n",
    "        if c.isalpha() == True:\n",
    "            if ord(c) >= 65 and ord(c)<=90:\n",
    "                caps = True\n",
    "            c = c.lower()\n",
    "            if c == prev:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "                prev = c\n",
    "            if count >= 3:\n",
    "                continue\n",
    "            if caps == True:\n",
    "                tweet_new += c.upper()\n",
    "            else:\n",
    "                tweet_new += c\n",
    "        else:\n",
    "            tweet_new += c\n",
    "    return tweet_new\n",
    "\n",
    "\n",
    "def Expand_Contractions(text):\n",
    "    return list(cont.expand_texts([text]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    new_text = text.apply(lambda x : list(x)).explode()\n",
    "    return new_text.unique().shape[0]\n",
    "\n",
    "def count_words(text):\n",
    "    new_text = text.apply(lambda x : x.split(' ')).explode()\n",
    "    return new_text.unique().shape[0]\n",
    "\n",
    "def preprocess_pipeline(steps, col, df):\n",
    "    new_col = df[col]\n",
    "    char_count_before = 0\n",
    "    word_count_before = 0\n",
    "    char_count_after = 0\n",
    "    word_count_after = 0\n",
    "    for each_step in steps:\n",
    "        char_count_before = count_chars(new_col)\n",
    "        word_count_before = count_words(new_col)\n",
    "        new_col = new_col.apply(each_step)\n",
    "        char_count_after = count_chars(new_col)\n",
    "        word_count_after = count_words(new_col)\n",
    "        print(\"Preprocessing step: \",each_step.__name__)\n",
    "        print(\"Char Count ---> Before: %d | After: %d\"%(char_count_before, char_count_after))\n",
    "        print(\"Word Count ---> Before: %d | After: %d\"%(word_count_before, word_count_after))\n",
    "    \n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "\n",
    "\n",
    "pipeline = []\n",
    "# pipeline.append(to_lower)\n",
    "pipeline.append(remove_newline)\n",
    "pipeline.append(remove_URL)\n",
    "pipeline.append(remove_html)\n",
    "pipeline.append(remove_emoji)\n",
    "# pipeline.append(hashtag_to_words)\n",
    "# pipeline.append(remove_words_with_numbers)\n",
    "# pipeline.append(remove_numbers)\n",
    "pipeline.append(remove_mentions)\n",
    "pipeline.append(remove_square_bracket)\n",
    "pipeline.append(remove_angular_bracket)\n",
    "pipeline.append(Expand_Contractions)\n",
    "# pipeline.append(remove_punctuations)\n",
    "pipeline.append(removeRepeated)\n",
    "# pipeline.append(convert_abbrev_in_text)\n",
    "# pipeline.append(remove_stopwords)\n",
    "# pipeline.append(correct_misspelled_with_context)\n",
    "# pipeline.append(remove_numbers)\n",
    "# pipeline.append(remove_stopwords)\n",
    "# # pipeline.append(stemming_text)\n",
    "pipeline.append(lemmatization)\n",
    "\n",
    "train['processed_text'] = preprocess_pipeline(pipeline, 'text', train)\n",
    "test['processed_text'] = preprocess_pipeline(pipeline, 'text', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()\n",
    "df_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "\n",
    "X_all = pd.concat([df[\"processed_text\"], df_test[\"processed_text\"]])\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "tfidf.fit(X_all)\n",
    "\n",
    "X = tfidf.transform(df[\"processed_text\"])\n",
    "X_test = tfidf.transform(df_test[\"processed_text\"])\n",
    "del X_all\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "train_x = train[\"processed_text\"]\n",
    "train_y = train[\"target\"]\n",
    "\n",
    "test_x = test[\"processed_text\"]\n",
    "test_y = test[\"target\"]\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \n",
    "    'gamma': [0.001, 0.01, 0.1, 0.4, 0.3, 0.6, 0.7, 1], \n",
    "    'kernel': ['rbf'], \n",
    "    'C': [0.001, 0.01, 0.1, 1, 1.3, 2, 3, 10],\n",
    "}\n",
    "model = GridSearchCV(SVC(), parameters, cv=10, n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cv_results_['params'][model.best_index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict(X_val)\n",
    "accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(os.path.join('../input/nlp-getting-started/', 'sample_submission.csv'))\n",
    "sub_df[\"target\"] = y_test_pred\n",
    "sub_df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
